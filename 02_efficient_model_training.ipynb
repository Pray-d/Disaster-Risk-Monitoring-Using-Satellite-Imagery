{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f36efa-b9f8-4935-a403-a4010cdbad8e",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/combined_logo.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ee592-705e-49c5-a26c-dd7cacfbb545",
   "metadata": {},
   "source": [
    "# Disaster Risk Monitoring Using Satellite Imagery #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a48909-8799-4b25-bd11-91cc92fb46a1",
   "metadata": {},
   "source": [
    "## 02 - Efficient Model Training ##\n",
    "In this notebook, you will learn how to train a segmentation model with the TAO Toolkit using pre-trained ResNet-18 weights. In addition, you will learn how to export the model for deployment. \n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This notebook covers the below sections: \n",
    "1. [Introduction to the TAO Toolkit](#s2-1)\n",
    "    * [Transfer Learning](#s2-1.1)\n",
    "    * [Vision AI Pre-Trained Models Supported](#s2-1.2)\n",
    "    * [TAO Toolkit Workflow](#s2-1.3)\n",
    "    * [TAO Launcher, CLI (Command Line Interface), and Spec Files](#s2-1.4)\n",
    "    * [Set Up Environment Variables](#s2-1.5)\n",
    "    * [Exercise #1 - Explore TAO Toolkit CLI](#s2-e1)\n",
    "2. [U-Net Semantic Segmentation Model](#s2-2)\n",
    "    * [Preparation for Model Training](#s2-2.1)\n",
    "    * [Download Pre-Trained Model](#s2-2.2)\n",
    "    * [Prepare Dataset](#s2-2.3)\n",
    "    * [Model Training](#s2-2.4)\n",
    "    * [Exercise #2 - Modify Dataset Config](#s2-e2)\n",
    "    * [Exercise #3 - Modify Model Config](#s2-e3)\n",
    "    * [Exercise #4 - Modify Training Config](#s2-e4)\n",
    "    * [Combine Configuration Files](#s2-2.5)\n",
    "    * [Initiate Model Training](#s2-2.6)\n",
    "    * [Evaluating the Model](#s2-2.7)\n",
    "    * [Visualizing Model Inference](#s2-2.8)\n",
    "3. [Model Export](#s2-3)\n",
    "    * [TensorRT - Programmable Inference Accelerator](#s2-3.1)\n",
    "    * [Export the Trained Model](#s2-3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae3a29-df70-4396-a6f4-d1d8d164d5ff",
   "metadata": {},
   "source": [
    "<a name='s2-1'></a>\n",
    "## Introduction to the TAO Toolkit ##\n",
    "The TAO Toolkit, Train Adapt Optimize, is a framework that simplifies the AI/ML model development workflow. It lets developers fine-tune pre-trained models with custom data to produce highly accurate computer vision models efficiently, eliminating the need for large training runs and deep AI expertise. In addition, it also enables model optimization for inference performance. You can learn more about the TAO Toolkit [here](https://developer.nvidia.com/tao-toolkit) or read the documentation [here](https://docs.nvidia.com/tao/tao-toolkit/index.html#).\n",
    "<p><img src=\"images/tao_toolkit.png\" width=720></p>\n",
    "\n",
    "The TAO Toolkit uses pre-trained models to accelerate the AI development process and reduce costs associated with large scale data collection, labeling, and training models from scratch. Transfer learning with pre-trained models can be used for classification, object detection, and image segmentation tasks. The TAO Toolkit offers useful features such as:\n",
    "* Low-coding approach that requires no AI framework expertise, reducing the barrier of entry for anyone who wants to get started building AI-based applications\n",
    "* Flexible configurations that allow customization to help advance users prototype faster\n",
    "* Large catalogue of production-ready pre-trained models for common Computer Vision (CV) tasks that can also be customized with users' own data\n",
    "* Easy to use interface for model optimization such as pruning and quantization-aware training\n",
    "* Integration with the Triton Inference Server\n",
    "\n",
    "_Note: The TAO Toolkit comes with a set of reference scripts and configuration specifications with default parameter values that enable developers to kick-start training and fine-tuning. This lowers the bar and enables users without a deep understanding of models, expertise in deep learning, or beginning coding skills to be able to train new models and fine-tune the pre-trained ones._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8aca25-bc7a-4dc7-a4d3-b903cbdf13f4",
   "metadata": {},
   "source": [
    "<a name='s2-1.1'></a>\n",
    "### Transfer Learning ###\n",
    "In practice, it is rare and inefficient to initiate the learning task on a network with randomly initialized weights due to factors like data scarcity (inadequate number of training samples) or prolonged training times. One of the most common techniques to overcome this is to use transfer learning. Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where developers use a model trained on one task and re-train to use it on a different task. This works surprisingly well as many of the early layers in a neural network are the same for similar tasks. For example, many of the early layers in a convolutional neural network used for a CV model are primarily used to identify outlines, curves, and other features in an image. The network formed by these layers are referred to as the **backbone** of a more complex model. Also known as feature extractors, they take as input the image and extracts the feature map upon which the rest of the network is based. The learned features from these layers can be applied to similar tasks carrying out the same identification in other domains. Transfer learning enables adaptation (fine-tuning) of an existing neural network to a new one, which requires significantly less domain-specific data. In most cases, fine-tuning takes significantly less time (a reduction by x10 factor is common), saving time and resources. As it relates to vision AI, transfer learning can be used for adding new classification by transferring weights from one application to another.\n",
    "\n",
    "<p><img src='images/transfer_learning.png' width=720></p>\n",
    "\n",
    "More information about transfer learning can be found in this [blog](https://blogs.nvidia.com/blog/2019/02/07/what-is-transfer-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb01061-8a05-4c4f-96aa-28926278b503",
   "metadata": {},
   "source": [
    "<a name='s2-1.2'></a>\n",
    "### Vision AI Pre-Trained Models Supported ###\n",
    "Developers, system builders, and software partners building disaster risk monitoring systems can bring their own custom data to train with and fine-tune pre-trained models quickly instead of going through significant effort in large data collection and training from scratch. **General purpose vision models** provide pre-trained weights for popular network architectures to train an image classification model, an object detection model, or a segmentation model. This gives users the flexibility and control to build AI models for any number of applications, from smaller lightweight models for edge deployment to larger models for more complex tasks. They are trained on the [Open Images](https://opensource.google/projects/open-images-dataset) dataset and provide a much better starting point for training versus training from scratch or starting from random weights. \n",
    "\n",
    "The TAO Toolkit adapts popular network architectures and backbones to custom data, allowing developers to train, fine tune, prune, and export highly optimized and accurate AI models. When working with TAO, first choose the model architecture to be built, then choose one of the supported backbones. \n",
    "<p><img src='images/openimage_table.jpg' width=720></p>\n",
    "\n",
    "_Note: The pre-trained weights from each feature extraction network merely act as a starting point and may not be used without re-training. In addition, the pre-trained weights are network specific and shouldn't be shared across models that use different architectures._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319ca49-760b-4aa1-b2f5-8c31657c125c",
   "metadata": {},
   "source": [
    "<a name='s2-1.3'></a>\n",
    "### TAO Toolkit Workflow ###\n",
    "Building disaster risk monitoring systems is hard, and tailoring even a single component to the needs of the enterprise for deployment is even harder. Deployment for a domain-specific application typically requires several cycles of re-training, fine-tuning, and deploying the model until it satisfies the requirements. As a starting point, training typically follows the below steps: \n",
    "\n",
    "0. Configuration\n",
    "1. Download a pre-trained model from NGC\n",
    "2. Prepare the data for training\n",
    "3. Train the model using transfer learning\n",
    "4. Evaluate the model for target predictions\n",
    "5. Export the model\n",
    "* Steps to optimize the model for improved inference performance\n",
    "\n",
    "<p><img src='images/tao_toolkit_workflow.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb69288-90df-455f-b53a-0a32adf74dbc",
   "metadata": {},
   "source": [
    "<a name='s2-1.4'></a>\n",
    "### TAO Launcher, CLI (Command Line Interface), and Spec Files ###\n",
    "The TAO Toolkit is a low-coding framework that makes it easy to get started. It uses a **launcher** to pull from NGC registry and instantiate the appropriate TAO [container](https://www.docker.com/resources/what-container/) that performs the desired subtasks such as convert data, train, evaluate, or export. The TAO launcher is a python package distributed as a python wheel listed in the `nvidia-pyindex` python index, which has been prepared for you already. \n",
    "\n",
    "Users interact with the launcher with its **Command Line Interface** that is configured using simple [**Protocol Buffer**](https://developers.google.com/protocol-buffers) **specification files** to include parameters such as the dataset parameters, model parameters, and optimizer and training hyperparameters. More information about the TAO Toolkit Launcher can be found in the [TAO Docs](https://docs.nvidia.com/tao/tao-toolkit/text/tao_launcher.html#tao-launcher). \n",
    "\n",
    "The tasks can be invoked from the TAO Toolkit Launcher using the convention `tao <task> <subtask> <args_per_subtask>`, where `<args_per_subtask>` are the arguments required for a given subtask. Once the container is launched, the subtasks are run by the TAO Toolkit containers using the appropriate hardware resources. \n",
    "<p><img src='images/tao_launcher.gif' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f316b3-e7b5-4a3a-8cc8-8f6c8f6f00a0",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "Since the TAO Toolkit uses the launcher to pull containers, the first time running a task may take extra time to load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f7f4ec-1826-499f-b001-28d1eb0d71a1",
   "metadata": {},
   "source": [
    "<a name='s2-1.5'></a>\n",
    "### Set Up Environment Variables ###\n",
    "We set up a couple of environment variables to help us mount the local directories to the tao container. Specifically, we want to set paths for the `$LOCAL_TRAINING_DATA`, `$LOCAL_SPEC_DIR`, and `$LOCAL_PROJECT_DIR` for the output of the TAO experiment with their respective paths in the TAO container. In doing so, we can make sure that the TAO experiment generated collaterals such as checkpoints, model files (e.g. `.tlt` or `.etlt`), and logs are output to `$LOCAL_PROJECT_DIR/unet`. \n",
    "\n",
    "_Note that users will be able to define their own export encryption key when training from a general-purpose model. This is to protect proprietary IP and used to decrypt the `.etlt` model during deployment._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446943db-e633-4b5b-9997-c767a849085d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KEY=my_model_key\n",
      "env: LOCAL_PROJECT_DIR=/dli/task/tao_project\n",
      "env: LOCAL_DATA_DIR=/dli/task/flood_data\n",
      "env: LOCAL_SPECS_DIR=/dli/task/tao_project/spec_files\n",
      "env: TAO_PROJECT_DIR=/workspace/tao-experiments\n",
      "env: TAO_DATA_DIR=/workspace/tao-experiments/data\n",
      "env: TAO_SPECS_DIR=/workspace/tao-experiments/spec_files\n",
      "env: TAO_EXPERIMENT_DIR=/workspace/tao-experiments/unet\n",
      "mkdir: cannot create directory ‘/dli/task/tao_project/unet’: File exists\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# set environment variables\n",
    "import os\n",
    "import json\n",
    "\n",
    "%set_env KEY=my_model_key\n",
    "\n",
    "%set_env LOCAL_PROJECT_DIR=/dli/task/tao_project\n",
    "%set_env LOCAL_DATA_DIR=/dli/task/flood_data\n",
    "%set_env LOCAL_SPECS_DIR=/dli/task/tao_project/spec_files\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"]=os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\"), \"unet\")\n",
    "\n",
    "%set_env TAO_PROJECT_DIR=/workspace/tao-experiments\n",
    "%set_env TAO_DATA_DIR=/workspace/tao-experiments/data\n",
    "%set_env TAO_SPECS_DIR=/workspace/tao-experiments/spec_files\n",
    "%set_env TAO_EXPERIMENT_DIR=/workspace/tao-experiments/unet\n",
    "\n",
    "!mkdir $LOCAL_EXPERIMENT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe38326-8dc7-438f-b392-913166d3a185",
   "metadata": {},
   "source": [
    "The cell below maps the project directory on your local host to a workspace directory in the TAO docker instance, so that the data and the results are mapped from in and out of the docker. This is done by creating a `.tao_mounts.json` file. For more information, please refer to the [launcher instance](https://docs.nvidia.com/tao/tao-toolkit/tao_launcher.html) in the user guide. Setting the `DockerOptions` ensures that you don't have permission issues when writing data into folders created by the TAO docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a30e1d-a66d-4752-85d7-6676f31c3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# mapping up the local directories to the TAO docker\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "            # Mapping the data directory\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "                \"destination\": \"/workspace/tao-experiments\"\n",
    "            },\n",
    "            # Mapping the specs directory.\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "                \"destination\": os.environ[\"TAO_SPECS_DIR\"]\n",
    "            },\n",
    "            # Mapping the data directory.\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_DATA_DIR\"],\n",
    "                \"destination\": os.environ[\"TAO_DATA_DIR\"]\n",
    "            },\n",
    "        ],\n",
    "    \"DockerOptions\": {\n",
    "        \"user\": \"{}:{}\".format(os.getuid(), os.getgid())\n",
    "    }\n",
    "}\n",
    "\n",
    "# writing the mounts file\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f9528-4a82-4e03-8719-55b30ff8e546",
   "metadata": {},
   "source": [
    "To see the usage of different functionality that are supported, use the `-h` or `--help` option. For more information, see the [TAO Toolkit Quick Start Guide](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_quick_start_guide.html). \n",
    "Here is the **sample output**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d2c2ed9-6c06-4e74-aa9e-1dc3034228c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tao [-h]\n",
      "           {list,stop,info,action_recognition,augment,bpnet,classification,converter,detectnet_v2,dssd,efficientdet,emotionnet,faster_rcnn,fpenet,gazenet,gesturenet,heartratenet,intent_slot_classification,lprnet,mask_rcnn,multitask_classification,n_gram,punctuation_and_capitalization,question_answering,retinanet,spectro_gen,speech_to_text,speech_to_text_citrinet,ssd,text_classification,token_classification,unet,vocoder,yolo_v3,yolo_v4,yolo_v4_tiny}\n",
      "           ...\n",
      "\n",
      "Launcher for TAO Toolkit.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "tasks:\n",
      "  {list,stop,info,action_recognition,augment,bpnet,classification,converter,detectnet_v2,dssd,efficientdet,emotionnet,faster_rcnn,fpenet,gazenet,gesturenet,heartratenet,intent_slot_classification,lprnet,mask_rcnn,multitask_classification,n_gram,punctuation_and_capitalization,question_answering,retinanet,spectro_gen,speech_to_text,speech_to_text_citrinet,ssd,text_classification,token_classification,unet,vocoder,yolo_v3,yolo_v4,yolo_v4_tiny}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!tao --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24d50a-f760-483c-90e0-680a45869b26",
   "metadata": {},
   "source": [
    "With the TAO Toolkit, users can train models for object detection, classification, segmentation, optical character recognition, facial landmark estimation, gaze estimation, and more. In TAO's terminology, these would be the **tasks**, which support **subtasks** such as `train`, `prune`, `evaluate`, `export`, etc. Each task/subtask requires different combinations of configuration files to accommodate for different parameters, such as the dataset parameters, model parameters, and optimizer and training hyperparameters. Part of what makes TAO Toolkit so easy to use is that most of those parameters are hidden away in the form of experiment specification files (spec files). They are detailed in the [documentation](https://docs.nvidia.com/tao/tao-toolkit/#tao-toolkit) for reference. It's very helpful to have these resources handy when working with the TAO Toolkit. In addition, there are several specific tasks that help with handling the launched commands. \n",
    "\n",
    "Below are the tasks available in the TAO Toolkit, organized by their respective computer vision objectives. We grayed out the tasks for Conversational AI as they are out of scope for this course.\n",
    "\n",
    "<img src='images/tao_tasks.png' width=740>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81e20c-14fa-4b8f-a630-ed8d9a509b59",
   "metadata": {},
   "source": [
    "<a name='s2-e1'></a>\n",
    "### Exercise #1 - Explore TAO Toolkit CLI ###\n",
    "Let's explore some TAO Toolkit tasks. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `<FIXME>`s only and execute the cell, choosing a task from options such as: `[classification, detectnet_v2, mask_rcnn, emotionnet, etc.]`, followed by a subtask from options such as: `[calibration_tensorfile, dataset_convert, evaluate, export, inference, prune, train]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce6698ea-f544-417e-9f19-9f205b473e55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `<'\n",
      "/bin/bash: -c: line 0: `tao <<<<detectnet_v2>>>> <<<<trainE>>>> --help'\n"
     ]
    }
   ],
   "source": [
    "# Example: !tao unet train --help\n",
    "!tao <<<<detectnet_v2>>>> <<<<trainE>>>> --help"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bac54013-b78f-4c84-9753-b409e9b019d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "!tao unet train --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf953065-06a7-491f-9fef-f5c2c24f7f8b",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f19dad-30cb-4d12-a15d-7268743ab2b3",
   "metadata": {},
   "source": [
    "<p><img src='images/check.png' width=720></p>\n",
    "\n",
    "Did you get the below error message? This is likely due to a bad NGC configuration. Please check the NGC CLI and Docker Registry section of the [introduction notebook](00_introduction.ipynb)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff968ed5-77ee-4237-8709-f55e96cc70b0",
   "metadata": {},
   "source": [
    "AssertionError: Config path must be a valid unix path. No file found at: /root/.docker/config.json. Did you run docker login?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f932728-0ae0-40bf-82b7-32f7b935cf8d",
   "metadata": {},
   "source": [
    "<a name='s2-2'></a>\n",
    "## U-Net Semantic Segmentation Model ##\n",
    "[U-Net](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pretrained_semantic_segmentation) is a neural network for image segmentation. This is the type of task we want to perform for our disaster risk monitoring system in order to label each pixel as either `flood` or `notflood`. With the TAO Toolkit, we can choose the desired ResNet-18 backbone as a feature extractor. As such, we will use the `unet` task, which supports the following subtasks: \n",
    "* `train`\n",
    "* `evaluate`\n",
    "* `inference`\n",
    "* `prune`\n",
    "* `export`\n",
    "\n",
    "<p><img src='images/rewind.png' width=720><p>\n",
    "    \n",
    "These subtasks can be invoked using the convention `tao unet <subtask> <args_per_subtask>` on the command-line, where `args_per_subtask` are the arguments required for a given subtask. Additionally, we can always find more information about these subtasks with `tao unet <subtask> -h` or `tao unet <subtask> --help`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281849b7-b93b-4d41-9437-8e34a2953087",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2-2.1'></a>\n",
    "### Preparation for Model Training ###\n",
    "For the remaining of the notebook, we will use the TAO Toolkit to train a semantic segmentation model. Below is what the model development workflow looks like. We start by preparing a pre-trained model and the data. Next, we prepare the configuration file(s) and begin to train the model with new data and evaluate its performance. We will export the model once its satisfactory. Note that this notebook does not include inference optimization steps, which is important for disaster risk monitoring systems that are deployed on edge devices. \n",
    "<p><img src='images/simple_workflow.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0bde40-85ba-472c-9984-6222fbf4d3d9",
   "metadata": {},
   "source": [
    "<a name='s2-2.2'></a>\n",
    "### Download Pre-Trained Model ###\n",
    "Developers typically begin by choosing and downloading a pre-trained model from [NGC](https://ngc.nvidia.com/), which contains pre-trained weights of the architecture of their choice. It's difficult to immediately identify which model/architecture will work best for a specific use case as there is often a tradeoff between time to train, accuracy, and inference performance. It is common to compare across multiple models before picking the best candidate.\n",
    "\n",
    "Here are some pointers that will help choose an appropriate model: \n",
    "* Look at the model inputs/outputs to decide if it will fit your use case. \n",
    "* Input format is also an important consideration. For example, some models expect the input to be 0-1 normalized with input channels in RGB order. Some models that use a different input order may require input preprocessing/mean subtraction that might result in suboptimal performance. \n",
    "\n",
    "We can use the `ngc registry model list <model_glob_string>` command to get a list of models that are hosted on the NGC model registry. For example, we can use `ngc registry model list nvidia/tao/*` to list all available models. The `--column` option identifies the columns of interest. More information about the NGC Registry CLI can be found in the [User Guide](https://docs.nvidia.com/dgx/pdf/ngc-registry-cli-user-guide.pdf). `The ngc registry model download-version <org>/[<team>/]<model-name:version>` command will download the model from the registry. It has a `--dest` option to specify the path to download directory. Alternatively, a catalog of support models can also be found [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/collections/tao_computervision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6a84c4-b633-41d1-a34d-7cd70282f348",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "    \"accuracyReached\": 77.56,\n",
      "    \"batchSize\": 1,\n",
      "    \"createdByUser\": \"n90fe0en2gvll5957fel7u75sg\",\n",
      "    \"createdDate\": \"2021-08-24T13:43:25.180Z\",\n",
      "    \"description\": \"\",\n",
      "    \"gpuModel\": \"V100\",\n",
      "    \"memoryFootprint\": \"153.7169\",\n",
      "    \"numberOfEpochs\": 80,\n",
      "    \"status\": \"UPLOAD_COMPLETE\",\n",
      "    \"totalFileCount\": 1,\n",
      "    \"totalSizeInBytes\": 161183816,\n",
      "    \"versionId\": \"vgg19\"\n",
      "},{\n",
      "    \"accuracyReached\": 82.2,\n",
      "    \"batchSize\": 1,\n",
      "    \"createdByUser\": \"n90fe0en2gvll5957fel7u75sg\",\n",
      "    \"createdDate\": \"2021-08-24T13:43:56.685Z\",\n",
      "    \"description\": \"\",\n",
      "    \"gpuModel\": \"V100\",\n",
      "    \"memoryFootprint\": \"515.0932\",\n",
      "    \"numberOfEpochs\": 80,\n",
      "    \"status\": \"UPLOAD_COMPLETE\",\n",
      "    \"totalFileCount\": 1,\n",
      "    \"totalSizeInBytes\": 540114376,\n",
      "    \"versionId\": \"vgg16\"\n",
      "},{\n",
      "    \"accuracyReached\": 77.91,\n",
      "    \"batchSize\": 1,\n",
      "    \"createdByUser\": \"n90fe0en2gvll5957fel7u75sg\",\n",
      "    \"createdDate\": \"2021-08-24T13:44:52.409Z\",\n",
      "    \"description\": \"\",\n",
      "    \"gpuModel\": \"V100\",\n",
      "    \"memoryFootprint\": \"294.1976\",\n",
      "    \"numberOfEpochs\": 80,\n",
      "    \"status\": \"UPLOAD_COMPLETE\",\n",
      "    \"totalFileCount\": 1,\n",
      "    \"totalSizeInBytes\": 308488496,\n",
      "    \"versionId\": \"resnet50\"\n",
      "},{\n",
      "    \"accuracyReached\": 77.04,\n",
      "    \"batchSize\": 1,\n",
      "    \"createdByUser\": \"n90fe0en2gvll5957fel7u75sg\",\n",
      "    \"createdDate\": \"2021-08-24T13:45:22.007Z\",\n",
      "    \"description\": \"\",\n",
      "    \"gpuModel\": \"V100\",\n",
      "    \"memoryFootprint\": \"170.6549\",\n",
      "    \"numberOfEpochs\": 80,\n",
      "    \"status\": \"UPLOAD_COMPLETE\",\n",
      "    \"totalFileCount\": 1,\n",
      "    \"totalSizeInBytes\": 178944632,\n",
      "    \"versionId\": \"resnet34\"\n",
      "},{\n",
      "    \"accuracyReached\": 76.74,\n",
      "    \"batchSize\": 1,\n",
      "    \"createdByUser\": \"n90fe0en2gvll5957fel7u75sg\",\n",
      "    \"createdDate\": \"2021-08-24T13:45:42.209Z\",\n",
      "    \"description\": \"\",\n",
      "    \"gpuModel\": \"V100\",\n",
      "    \"memoryFootprint\": \"88.9573\",\n",
      "    \"numberOfEpochs\": 80,\n",
      "    \"status\": \"UPLOAD_COMPLETE\",\n",
      "    \"totalFileCount\": 1,\n",
      "    \"totalSizeInBytes\": 93278448,\n",
      "    \"versionId\": \"resnet18\"\n",
      "},{\n",
      "    \"accuracyReached\": 77.78,\n",
      "    \"batchSize\": 1,\n",
      "    \"createdByUser\": \"n90fe0en2gvll5957fel7u75sg\",\n",
      "    \"createdDate\": \"2021-08-24T13:45:54.627Z\",\n",
      "    \"description\": \"\",\n",
      "    \"gpuModel\": \"V100\",\n",
      "    \"memoryFootprint\": \"576.3329\",\n",
      "    \"numberOfEpochs\": 80,\n",
      "    \"status\": \"UPLOAD_COMPLETE\",\n",
      "    \"totalFileCount\": 1,\n",
      "    \"totalSizeInBytes\": 604328880,\n",
      "    \"versionId\": \"resnet101\"\n",
      "},{\n",
      "    \"accuracyReached\": 74.38,\n",
      "    \"batchSize\": 1,\n",
      "    \"createdByUser\": \"n90fe0en2gvll5957fel7u75sg\",\n",
      "    \"createdDate\": \"2021-08-24T13:47:01.252Z\",\n",
      "    \"description\": \"\",\n",
      "    \"gpuModel\": \"V100\",\n",
      "    \"memoryFootprint\": \"38.3121\",\n",
      "    \"numberOfEpochs\": 80,\n",
      "    \"status\": \"UPLOAD_COMPLETE\",\n",
      "    \"totalFileCount\": 1,\n",
      "    \"totalSizeInBytes\": 40173128,\n",
      "    \"versionId\": \"resnet10\"\n",
      "},{\n",
      "    \"accuracyReached\": 77.11,\n",
      "    \"batchSize\": 1,\n",
      "    \"createdByUser\": \"n90fe0en2gvll5957fel7u75sg\",\n",
      "    \"createdDate\": \"2021-08-24T13:47:19.420Z\",\n",
      "    \"description\": \"\",\n",
      "    \"gpuModel\": \"V100\",\n",
      "    \"memoryFootprint\": \"16.9008\",\n",
      "    \"numberOfEpochs\": 80,\n",
      "    \"status\": \"UPLOAD_COMPLETE\",\n",
      "    \"totalFileCount\": 1,\n",
      "    \"totalSizeInBytes\": 17721744,\n",
      "    \"versionId\": \"efficientnet_b0_swish\"\n",
      "},{\n",
      "    \"accuracyReached\": 77.11,\n",
      "    \"batchSize\": 1,\n",
      "    \"createdByUser\": \"n90fe0en2gvll5957fel7u75sg\",\n",
      "    \"createdDate\": \"2021-08-24T13:47:35.558Z\",\n",
      "    \"description\": \"\",\n",
      "    \"gpuModel\": \"V100\",\n",
      "    \"memoryFootprint\": \"16.9008\",\n",
      "    \"numberOfEpochs\": 80,\n",
      "    \"status\": \"UPLOAD_COMPLETE\",\n",
      "    \"totalFileCount\": 1,\n",
      "    \"totalSizeInBytes\": 17721744,\n",
      "    \"versionId\": \"efficientnet_b0_relu\"\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# list all available models\n",
    "!ngc registry model list nvidia/tao/pretrained_semantic_segmentation:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334806b-5dd7-4c36-b1a4-b2bfe2620ffe",
   "metadata": {},
   "source": [
    "<p><img src='images/check.png' width=720></p>\n",
    "\n",
    "Did you get the below error message? This is likely due to a bad NGC CLI configuration. Please check the NGC CLI and Docker Registry section of the [introduction notebook](00_introduction.ipynb)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d0d9d37-d76a-420e-bfea-f1a05e902488",
   "metadata": {},
   "source": [
    "{\n",
    "    \"error\": \"Error: Invalid apikey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dffefc95-2e28-4d30-bcb1-64bc1a8fdf33",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"download_end\": \"2023-09-24 16:33:33.456390\",\n",
      "    \"download_start\": \"2023-09-24 16:33:30.452243\",\n",
      "    \"download_time\": \"3s\",\n",
      "    \"files_downloaded\": 1,\n",
      "    \"local_path\": \"/dli/task/tao_project/unet/pretrained_resnet18/pretrained_semantic_segmentation_vresnet18\",\n",
      "    \"size_downloaded\": \"82.38 MB\",\n",
      "    \"status\": \"Completed\",\n",
      "    \"transfer_id\": \"pretrained_semantic_segmentation_vresnet18\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# create directory to store the pre-trained model\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/\n",
    "\n",
    "# download the pre-trained segmentation model from NGC\n",
    "!ngc registry model download-version nvidia/tao/pretrained_semantic_segmentation:resnet18 \\\n",
    "    --dest $LOCAL_EXPERIMENT_DIR/pretrained_resnet18 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac3ffc-7afb-4369-804f-7614b0f639fa",
   "metadata": {},
   "source": [
    "For this lab, we will use _ResNet18_ as the architecture for the semantic segmentation model. [Residual neural network](https://en.wikipedia.org/wiki/Residual_neural_network), or **ResNet**, is a type of convolutional neural network used as a backbone for many computer vision tasks. The `18` refers to the number of layers in this architecture. It should be noted that typically the deeper (i.e. more layers) a neural network is, the more time consuming it is to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b747f9d-8788-4c77-a6f6-b8d5440e503e",
   "metadata": {},
   "source": [
    "<p><img src='images/tip.png' width=720></p>\n",
    "\n",
    "We designated the model to be downloaded locally to `tao_project/unet/pretrained_resnet18`, which is mapped to `/workspace/tao-experiments/unet/pretrained_resnet18` in the TAO container based on the mapping of `LOCAL_PROJECT_DIR` to `TAO_PROJECT_DIR`. Looking at the `local_path` and `transfer_id` keys of the output JSON, we can gather that the path of the pre-trained model should be in the `tao_project/unet/pretrained_resnet18/pretrained_semantic_segmentation_vresnet18 directory`. When referencing paths for the TAO Toolkit, it's important to use paths based on the TAO container. In this case it would be `/workspace/tao-experiments/unet/pretrained_resnet18/pretrained_semantic_segmentation_vresnet18`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b667fe1-928b-48df-b1fb-dfc762661035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtao_project/unet/pretrained_resnet18\u001b[00m\n",
      "└── \u001b[01;34mpretrained_semantic_segmentation_vresnet18\u001b[00m\n",
      "    └── resnet_18.hdf5\n",
      "\n",
      "1 directory, 1 file\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!tree -a tao_project/unet/pretrained_resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e1652-fe1f-4a17-9307-0d2353ab1945",
   "metadata": {},
   "source": [
    "<a name='s2-2.3'></a>\n",
    "### Prepare Dataset ###\n",
    "The TAO Toolkit expects the training data for the `unet` subtasks to be in the format described in the [documentation](https://docs.nvidia.com/tao/tao-toolkit/text/data_annotation_format.html#id8). Each mask image is a single-channel image, where every pixel is assigned an integer value that represents the segmentation class `label_id`, as per the mapping provided in the `dataset_config`. Additionally, each image and label have the same file ID before the extension and size. The image-to-label correspondence is maintained using this filename. The data folder structure for images and masks must be in the following format. \n",
    "<p><img src='images/semantic_segmentation_input.PNG' width=720></p>\n",
    "\n",
    "_The `test` folder in the above directory structure is optional; any folder can be used for inference._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89e3e5-08a5-4bda-95db-9697acf0694c",
   "metadata": {},
   "source": [
    "Below we will split the data into `train` set and `validation` set and copy the images into their respective folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af9ebbf0-1e1b-470e-9c3b-7fd9fcc62906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# remove existing splits\n",
    "!rm -rf $LOCAL_DATA_DIR/images/train\n",
    "!mkdir -p $LOCAL_DATA_DIR/images/train\n",
    "!rm -rf $LOCAL_DATA_DIR/images/val\n",
    "!mkdir -p $LOCAL_DATA_DIR/images/val\n",
    "\n",
    "!rm -rf $LOCAL_DATA_DIR/masks/train\n",
    "!mkdir -p $LOCAL_DATA_DIR/masks/train\n",
    "!rm -rf $LOCAL_DATA_DIR/masks/val\n",
    "!mkdir -p $LOCAL_DATA_DIR/masks/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d3a078-58db-42a6-b873-56d87f024ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "from random import sample\n",
    "import shutil\n",
    "\n",
    "# define split ratio\n",
    "split=0.75\n",
    "\n",
    "# get all images\n",
    "file_list=os.listdir(f\"{os.environ['LOCAL_DATA_DIR']}/images/all_images\")\n",
    "image_count=len(file_list)\n",
    "train_image_list=sample(file_list, int(image_count*split))\n",
    "val_image_list=[file for file in file_list if file not in train_image_list]\n",
    "\n",
    "# move all training images to train directory\n",
    "for each_file in train_image_list: \n",
    "    if each_file.split('.')[-1]=='png': \n",
    "        shutil.copyfile(f\"{os.environ['LOCAL_DATA_DIR']}/images/all_images/{each_file}\", f\"{os.environ['LOCAL_DATA_DIR']}/images/train/{each_file}\")\n",
    "        shutil.copyfile(f\"{os.environ['LOCAL_DATA_DIR']}/masks/all_masks/{each_file}\", f\"{os.environ['LOCAL_DATA_DIR']}/masks/train/{each_file}\")\n",
    "\n",
    "# move all validation images to val directory\n",
    "for each_file in val_image_list: \n",
    "    if each_file.split('.')[-1]=='png': \n",
    "        shutil.copyfile(f\"{os.environ['LOCAL_DATA_DIR']}/images/all_images/{each_file}\", f\"{os.environ['LOCAL_DATA_DIR']}/images/val/{each_file}\")\n",
    "        shutil.copyfile(f\"{os.environ['LOCAL_DATA_DIR']}/masks/all_masks/{each_file}\", f\"{os.environ['LOCAL_DATA_DIR']}/masks/val/{each_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b710aa-c92c-4d09-934f-5096b2711965",
   "metadata": {},
   "source": [
    "<a name='s2-2.4'></a>\n",
    "### Model Training ###\n",
    "Training configuration is done through a training spec file, which includes options such as which dataset to use for training, which dataset to use for validation, which pre-trained model architecture to use, which hyperparameters to tune, and other training options. The `train`, `evaluate`, `prune`, and `inference` subtasks for a U-Net experiment share the same configuration file. Configuration files can be created from scratch or modified using the templates provided in TAO Toolkit's [sample applications](https://docs.nvidia.com/tao/tao-toolkit/#cv-applications). \n",
    "\n",
    "The training configuration file has the following sections: \n",
    "* `dataset_config`\n",
    "* `model_config`\n",
    "* `training_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb34518-d2fd-449c-bcbf-fa5b2fcd086f",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "We will create the configuration files using templates. Specifically, we have broken the configuration files into separate parts for ease of discussion, which we will combine at the end for the TAO Toolkit to consume. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4241d-14b2-44fb-8820-83c3d61f93cd",
   "metadata": {},
   "source": [
    "Execute the below cells to preview the combined training/evaluation configuration file that will be used. It is currently not usable as we have made some intentional modifications that will require correction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3a3201c-2443-4f94-852f-f15cc51b25e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_config {\n",
      "  dataset: \"<<<<FIXME>>>>\"\n",
      "  augment: <<<<FIXME>>>>\n",
      "  augmentation_config {\n",
      "    spatial_augmentation {\n",
      "      hflip_probability : 0.5\n",
      "      vflip_probability : 0.5\n",
      "      crop_and_resize_prob : 0.5\n",
      "    }\n",
      "  }\n",
      "  input_image_type: \"<<<<FIXME>>>>\"\n",
      "  train_images_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "  train_masks_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "\n",
      "  val_images_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "  val_masks_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "  \n",
      "  test_images_path: \"/workspace/tao-experiments/data/images/val\"\n",
      "  \n",
      "  data_class_config {\n",
      "    target_classes {\n",
      "      name: \"notflood\"\n",
      "      mapping_class: \"notflood\"\n",
      "      label_id: 0\n",
      "    }\n",
      "    target_classes {\n",
      "      name: \"flood\"\n",
      "      mapping_class: \"flood\"\n",
      "      label_id: 255\n",
      "    }\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "model_config {\n",
      "  model_input_width: <<<<FIXME>>>>\n",
      "  model_input_height: <<<<FIXME>>>>\n",
      "  model_input_channels: <<<<FIXME>>>>\n",
      "  num_layers: 18\n",
      "  all_projections: true\n",
      "  arch: \"resnet\"\n",
      "  use_batch_norm: true\n",
      "  training_precision {\n",
      "    backend_floatx: FLOAT32\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "training_config {\n",
      "  batch_size: 1\n",
      "  epochs: <<<<FIXME>>>>\n",
      "  log_summary_steps: 10\n",
      "  checkpoint_interval: 10\n",
      "  loss: \"cross_dice_sum\"\n",
      "  learning_rate: 0.0001\n",
      "  regularizer {\n",
      "    type: L2\n",
      "    weight: 2e-5\n",
      "  }\n",
      "  optimizer {\n",
      "    adam {\n",
      "      epsilon: 9.99999993923e-09\n",
      "      beta1: 0.899999976158\n",
      "      beta2: 0.999000012875\n",
      "    }\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# combining configuration components in separate files and writing into one\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/dataset_config.txt \\\n",
    "     $LOCAL_SPECS_DIR/resnet18/model_config.txt \\\n",
    "     $LOCAL_SPECS_DIR/resnet18/training_config.txt \\\n",
    "     > $LOCAL_SPECS_DIR/resnet18/combined_config.txt\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/combined_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ded461-e7f3-4d20-b406-e1a0b309eb29",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "Note that we must leave an empty new line at the end of each text file to ensure the `combined_config.txt` is created properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b587b-d099-48ab-be87-3e3c8b3f41af",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2-e2'></a>\n",
    "### Exercise #2 - Modify Dataset Config ###\n",
    "The dataloader defines the path of the data that the model will train on and the class mapping for the classes in the dataset. We have previously generated images and masks for the training datasets. To use the newly generated training data, update the `dataset_config` parameter in the spec file to reference the correct directory. \n",
    "* `dataset (str)`: The input type dataset used. The currently supported dataset is `custom` to the user. Open-source datasets will be added in the future. \n",
    "* `augment (bool)`: If the input should be augmented online while training. When using one’s own dataset to train and fine-tune a model, the dataset can be augmented while training to introduce variations in the dataset. This is known as **online augmentation**. This is very useful in training as data variation improves the overall quality of the model and prevents [overfitting](https://en.wikipedia.org/wiki/Overfitting). Training a deep neural network requires large amounts of annotated data, which can be a manual and expensive process. Furthermore, it can be difficult to estimate all the corner cases that the network may go through. The TAO Toolkit provides _spatial augmentation_ (resize and flip) and _color space augmentation_ (brightness) to create synthetic data variations. \n",
    "* `augmentation_config (dict)`: \n",
    "    * `spatial_augmentation (dict)`: Supports spatial augmentation such as flip, zoom, and translate. \n",
    "        * `hflip_probability (float)`: Probability to flip an input image horizontally. \n",
    "        * `vflip_probability (float)`: Probability to flip an input image vertically. \n",
    "        * `crop_and_resize_prob (float)`\n",
    "    * `brightness_augmentation (dict)`: Configures the color space transformation. \n",
    "        * `delta (float)`: Adjust brightness using delta value. \n",
    "* `input_image_type (str)`: The input image type to indicate if input image is `grayscale` or `color` (RGB). \n",
    "* `train_images_path (str)`, `train_masks_path (str)`, `val_images_path (str)`, `val_masks_path (str)`, `test_images_path (str)`: The path string for train images, train masks, validation images, validation masks, and test images (optional). \n",
    "* `data_class_config (dict)`: Proto dictionary that contains information of training classes as part of target_classes proto which is described below.\n",
    "    * `target_classes (dict)`: The repeated field for every training class. The following are required parameters for the `target_classes` config:\n",
    "        * `name (str)`: The name of the target class. \n",
    "        * `mapping_class (str)`: The name of the mapping class for the target class. If the class needs to be trained as is, then name and mapping_class should be the same.\n",
    "        * `label_id (int)`: The pixel that belongs to this target class is assigned this label_id value in the mask image.\n",
    "\n",
    "_Note the supported image extension formats for training images are “.png”, “.jpg”, “.jpeg”, “.PNG”, “.JPG”, and “.JPEG”._\n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `dataset_config`[(here)](tao_project/spec_files/resnet18/dataset_config.txt) section of the training configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d107a8a2-ffd6-4bc3-9a89-1062afdd53f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_config {\n",
      "  dataset: \"<<<<FIXME>>>>\"\n",
      "  augment: <<<<FIXME>>>>\n",
      "  augmentation_config {\n",
      "    spatial_augmentation {\n",
      "      hflip_probability : 0.5\n",
      "      vflip_probability : 0.5\n",
      "      crop_and_resize_prob : 0.5\n",
      "    }\n",
      "  }\n",
      "  input_image_type: \"<<<<FIXME>>>>\"\n",
      "  train_images_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "  train_masks_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "\n",
      "  val_images_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "  val_masks_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "  \n",
      "  test_images_path: \"/workspace/tao-experiments/data/images/val\"\n",
      "  \n",
      "  data_class_config {\n",
      "    target_classes {\n",
      "      name: \"notflood\"\n",
      "      mapping_class: \"notflood\"\n",
      "      label_id: 0\n",
      "    }\n",
      "    target_classes {\n",
      "      name: \"flood\"\n",
      "      mapping_class: \"flood\"\n",
      "      label_id: 255\n",
      "    }\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# read the config file\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/dataset_config.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19bc4ae2-6f1d-4204-aecb-7316a1b33ec7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "dataset_config {\n",
    "  dataset: \"custom\"\n",
    "  augment: true\n",
    "  augmentation_config {\n",
    "    spatial_augmentation {\n",
    "      hflip_probability : 0.5\n",
    "      vflip_probability : 0.5\n",
    "      crop_and_resize_prob : 0.5\n",
    "    }\n",
    "  }\n",
    "  input_image_type: \"color\"\n",
    "  train_images_path:\"/workspace/tao-experiments/data/images/train\"\n",
    "  train_masks_path:\"/workspace/tao-experiments/data/masks/train\"\n",
    "\n",
    "  val_images_path:\"/workspace/tao-experiments/data/images/val\"\n",
    "  val_masks_path:\"/workspace/tao-experiments/data/masks/val\"\n",
    "  \n",
    "  test_images_path:\"/workspace/tao-experiments/data/images/val\"\n",
    "  \n",
    "  data_class_config {\n",
    "    target_classes {\n",
    "      name: \"notflood\"\n",
    "      mapping_class: \"notflood\"\n",
    "      label_id: 0\n",
    "    }\n",
    "    target_classes {\n",
    "      name: \"flood\"\n",
    "      mapping_class: \"flood\"\n",
    "      label_id: 255\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f334dfe-04fc-455a-a94d-9624cfb85c1a",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9eb38b-7041-4078-b4ad-60ea7086e2fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2-e3'></a>\n",
    "### Exercise #3 - Modify Model Config ###\n",
    "The segmentation model can be configured using the `model_config` option in the spec file. \n",
    "* `all_projections (bool)`: For templates with shortcut connections, this parameter defines whether all shortcuts should be instantiated with 1x1 projection layers, irrespective of a change in stride across the input and output. \n",
    "* `arch (str)`: The architecture of the backbone feature extractor to be used for training. \n",
    "* `num_layers (int)`: The depth of the feature extractor for scalable templates. \n",
    "* `use_batch_norm (bool)`: A Boolean value that determines whether to use batch normalization layers or not. \n",
    "* `training_precision (dict)`: Contains a nested parameter that sets the precision of the back-end training framework. \n",
    "    * `backend_floatx`: The back-end training framework should be set to `FLOAT322`. \n",
    "* `initializer (choice)`: Initialization of convolutional layers. Supported initializations are `HE_UNIFORM`, `HE_NORMAL`, and `GLOROT_UNIFORM`. \n",
    "* `model_input_height (int)`: The model input height dimension of the model, which should be a multiple of 16.\n",
    "* `model_input_width (int)`: The model input width dimension of the model, which should be a multiple of 16.\n",
    "* `model_input_channels (int)`: The model-input channels dimension of the model, which should be set to 3 for a ResNet/VGG backbone. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `model_config`[(here)](tao_project/spec_files/resnet18/model_config.txt) section of the configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5ac8ebc-0ede-4d6f-b982-47f6bf649b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_config {\n",
      "  model_input_width: <<<<FIXME>>>>\n",
      "  model_input_height: <<<<FIXME>>>>\n",
      "  model_input_channels: <<<<FIXME>>>>\n",
      "  num_layers: 18\n",
      "  all_projections: true\n",
      "  arch: \"resnet\"\n",
      "  use_batch_norm: true\n",
      "  training_precision {\n",
      "    backend_floatx: FLOAT32\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# read the config file\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/model_config.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3798c024-75c6-45cf-ab2f-39519e766747",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "model_config {\n",
    "  model_input_width: 512\n",
    "  model_input_height: 512\n",
    "  model_input_channels: 3\n",
    "  num_layers: 18\n",
    "  all_projections: true\n",
    "  arch: \"resnet\"\n",
    "  use_batch_norm: true\n",
    "  training_precision {\n",
    "    backend_floatx: FLOAT32\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7fc6a-fb9b-4238-8b33-f5621a0b650c",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc06f5-3044-43b6-a35d-f7338e21ea3d",
   "metadata": {},
   "source": [
    "<a name='s2-e4'></a>\n",
    "#### Exercise #4 - Modify Training Config ####\n",
    "The `training_config` describes the training and learning process. \n",
    "* `batch_size (int)`: The number of images per batch per gpu. \n",
    "* `epochs (int)`: The number of epochs to train the model. One epoch represents one iteration of training through the entire dataset. \n",
    "* `log_summary_steps (int)`: The summary-steps interval at which train details are printed to stdout. \n",
    "* `checkpoint_interval (int)`: The number of epochs interval at which the checkpoint is saved. \n",
    "* `loss (str)`: The loss to be used for segmentation. \n",
    "* `learning_rate (float)`: The learning-rate initialization value. \n",
    "* `regularizer (dict)`: This parameter configures the type and weight of the regularizer to be used during training. The two parameters include:\n",
    "    * `type (Choice)`: The type of the regularizer being used should be `L2` or `L1`. \n",
    "    * `weight (Float)`: The floating-point weight of the regularizer. \n",
    "* `optimizer (dict)`: This parameter defines which optimizer to use for training, and the parameters to configure it, namely:\n",
    "    * `adam`: \n",
    "        * `epsilon (float)`: Is a very small number to prevent any division by zero in the implementation. \n",
    "        * `beta1 (float)`.  \n",
    "        * `beta2 (float)`. \n",
    "* `activation (str)`: The activation to be used on the last layer supported is `softmax`. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `training_config`[(here)](tao_project/spec_files/resnet18/training_config.txt) section of the training configuration file by changing the `<FIXME>` into acceptable values and **save changes**. Typically, using a higher epochs count will improve model performance but takes longer time to complete. For the purpose of this exercise, we recommend starting with a low `n_epochs`, such as `10`, to allow the model to converge without taking too much time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f73fff69-46a3-46a3-ad07-cf7947238405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_config {\n",
      "  batch_size: 1\n",
      "  epochs: <<<<FIXME>>>>\n",
      "  log_summary_steps: 10\n",
      "  checkpoint_interval: 10\n",
      "  loss: \"cross_dice_sum\"\n",
      "  learning_rate: 0.0001\n",
      "  regularizer {\n",
      "    type: L2\n",
      "    weight: 2e-5\n",
      "  }\n",
      "  optimizer {\n",
      "    adam {\n",
      "      epsilon: 9.99999993923e-09\n",
      "      beta1: 0.899999976158\n",
      "      beta2: 0.999000012875\n",
      "    }\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# read the config file\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/training_config.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcdeb7d9-6d03-4614-8fec-c7e3dbc15988",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "training_config {\n",
    "  batch_size: 1\n",
    "  epochs: 10\n",
    "  log_summary_steps: 10\n",
    "  checkpoint_interval: 10\n",
    "  loss: \"cross_dice_sum\"\n",
    "  learning_rate: 0.0001\n",
    "  regularizer {\n",
    "    type: L2\n",
    "    weight: 2e-5\n",
    "  }\n",
    "  optimizer {\n",
    "    adam {\n",
    "      epsilon: 9.99999993923e-09\n",
    "      beta1: 0.899999976158\n",
    "      beta2: 0.999000012875\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35245c3-e8e1-4d16-a97b-c2cd97325f00",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac10ab-9555-42fa-bf53-8a4c81daaedc",
   "metadata": {},
   "source": [
    "<a name='s2-2.5'></a>\n",
    "### Combine Configuration Files ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67af8d63-2748-47b9-9bc1-ac561a34d6af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_config {\n",
      "  dataset: \"<<<<FIXME>>>>\"\n",
      "  augment: <<<<FIXME>>>>\n",
      "  augmentation_config {\n",
      "    spatial_augmentation {\n",
      "      hflip_probability : 0.5\n",
      "      vflip_probability : 0.5\n",
      "      crop_and_resize_prob : 0.5\n",
      "    }\n",
      "  }\n",
      "  input_image_type: \"<<<<FIXME>>>>\"\n",
      "  train_images_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "  train_masks_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "\n",
      "  val_images_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "  val_masks_path: \"/workspace/tao-experiments/data/<<<<FIXME>>>>\"\n",
      "  \n",
      "  test_images_path: \"/workspace/tao-experiments/data/images/val\"\n",
      "  \n",
      "  data_class_config {\n",
      "    target_classes {\n",
      "      name: \"notflood\"\n",
      "      mapping_class: \"notflood\"\n",
      "      label_id: 0\n",
      "    }\n",
      "    target_classes {\n",
      "      name: \"flood\"\n",
      "      mapping_class: \"flood\"\n",
      "      label_id: 255\n",
      "    }\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "model_config {\n",
      "  model_input_width: <<<<FIXME>>>>\n",
      "  model_input_height: <<<<FIXME>>>>\n",
      "  model_input_channels: <<<<FIXME>>>>\n",
      "  num_layers: 18\n",
      "  all_projections: true\n",
      "  arch: \"resnet\"\n",
      "  use_batch_norm: true\n",
      "  training_precision {\n",
      "    backend_floatx: FLOAT32\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "training_config {\n",
      "  batch_size: 1\n",
      "  epochs: <<<<FIXME>>>>\n",
      "  log_summary_steps: 10\n",
      "  checkpoint_interval: 10\n",
      "  loss: \"cross_dice_sum\"\n",
      "  learning_rate: 0.0001\n",
      "  regularizer {\n",
      "    type: L2\n",
      "    weight: 2e-5\n",
      "  }\n",
      "  optimizer {\n",
      "    adam {\n",
      "      epsilon: 9.99999993923e-09\n",
      "      beta1: 0.899999976158\n",
      "      beta2: 0.999000012875\n",
      "    }\n",
      "  }\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# combining configuration components in separate files and writing into one\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/dataset_config.txt \\\n",
    "     $LOCAL_SPECS_DIR/resnet18/model_config.txt \\\n",
    "     $LOCAL_SPECS_DIR/resnet18/training_config.txt \\\n",
    "     > $LOCAL_SPECS_DIR/resnet18/combined_config.txt\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/combined_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcd57b-41cf-432e-b2f3-033140bb8799",
   "metadata": {},
   "source": [
    "<a name='s2-2.6'></a>\n",
    "### Initiate Model Training ###\n",
    "After preparing input data and setting up a spec file, we can start training the semantic segmentation model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "71e40905-6482-48dc-b81b-a5a5a9657f99",
   "metadata": {},
   "source": [
    "tao unet train [-h] -e <EXPERIMENT_SPEC_FILE>\n",
    "                    -r <RESULTS_DIR>\n",
    "                    -n <MODEL_NAME>\n",
    "                    -m <PRETRAINED_MODEL_FILE>\n",
    "                    -k <key>\n",
    "                    [-v Set Verbosity of the logger]\n",
    "                    [--gpus GPUS]\n",
    "                    [--gpu_index GPU_INDEX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe0274-1990-48cf-b3d1-191d9e191f67",
   "metadata": {},
   "source": [
    "When using the `train` subtask, the `-e` argument indicates the path to the spec file, the `-r` argument indicates the result directory, and the `-k` indicates the key to _load_ the pre-trained weights. There are some arguments that might be useful such as `-n` to indicates the name of the final step model saved and `-m` to indicate the path to a pre-trained model to initialize. Based on how NGC names the model downloaded, we should use `/workspace/tao-experiments/unt/pretrained_resnet18/pretrained_semantic_segmentation_vresnet18/resnet_18.hdf5` to reference the pre-trained model.\n",
    "\n",
    "_Multi-GPU support can be enabled for those with the hardware using the `--gpus` argument. When running the training with more than one GPU, we will need to modify the `batch_size` and `learning_rate`. In most cases, scaling down the batch-size by a factor of NUM_GPU's or scaling up the learning rate by a factor of NUM_GPUs would be a good place to start._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9bd470c-9b7e-4aa2-b800-4e0734431b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# remove any previous training if exists\n",
    "!rm -rf $LOCAL_EXPERIMENT_DIR/resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8ba85-1a06-424e-a4ec-6461de7df0d2",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "While the TAO Toolkit is running, there may be some _TensorFlow deprecation_ warning messages that can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1683c791-8dd8-461c-8615-7dd4ebdac421",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-24 16:34:18,944 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-09-24 16:34:19,068 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-tf:v3.21.11-tf1.15.5-py3\n",
      "2023-09-24 16:34:19,069 [INFO] tlt.components.docker_handler.docker_handler: The required docker doesn't exist locally/the manifest has changed. Pulling a new docker.\n",
      "2023-09-24 16:34:19,069 [INFO] tlt.components.docker_handler.docker_handler: Pulling the required container. This may take several minutes if you're doing this for the first time. Please wait here.\n",
      "...\n",
      "Pulling from repository: nvcr.io/nvidia/tao/tao-toolkit-tf\n",
      "2023-09-24 16:42:06,984 [INFO] tlt.components.docker_handler.docker_handler: Container pull complete.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/hooks/checkpoint_saver_hook.py:21: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/hooks/pretrained_restore_hook.py:23: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/hooks/pretrained_restore_hook.py:23: The name tf.logging.WARN is deprecated. Please use tf.compat.v1.logging.WARN instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/train.py:410: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "Loading experiment spec at /workspace/tao-experiments/spec_files/resnet18/combined_config.txt.\n",
      "2023-09-24 16:42:14,187 [INFO] __main__: Loading experiment spec at /workspace/tao-experiments/spec_files/resnet18/combined_config.txt.\n",
      "2023-09-24 16:42:14,188 [INFO] iva.unet.spec_handler.spec_loader: Merging specification from /workspace/tao-experiments/spec_files/resnet18/combined_config.txt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1439, in ConsumeBool\n",
      "    result = ParseBool(self.token)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1756, in ParseBool\n",
      "    raise ValueError('Expected \"true\" or \"false\".')\n",
      "ValueError: Expected \"true\" or \"false\".\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/train.py\", line 424, in <module>\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/train.py\", line 418, in main\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/train.py\", line 288, in run_experiment\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/spec_handler/spec_loader.py\", line 68, in load_experiment_spec\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/spec_handler/spec_loader.py\", line 48, in load_proto\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/spec_handler/spec_loader.py\", line 34, in _load_from_file\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 725, in Merge\n",
      "    allow_unknown_field=allow_unknown_field)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 793, in MergeLines\n",
      "    return parser.MergeLines(lines, message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 818, in MergeLines\n",
      "    self._ParseOrMerge(lines, message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 837, in _ParseOrMerge\n",
      "    self._MergeField(tokenizer, message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 967, in _MergeField\n",
      "    merger(tokenizer, message, field)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1042, in _MergeMessageField\n",
      "    self._MergeField(tokenizer, sub_message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 967, in _MergeField\n",
      "    merger(tokenizer, message, field)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1091, in _MergeScalarField\n",
      "    value = tokenizer.ConsumeBool()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1441, in ConsumeBool\n",
      "    raise self.ParseError(str(e))\n",
      "google.protobuf.text_format.ParseError: 3:12 : '  augment: <<<<FIXME>>>>': Expected \"true\" or \"false\".\n",
      "2023-09-24 16:42:15,481 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# train model\n",
    "!tao unet train -e $TAO_SPECS_DIR/resnet18/combined_config.txt \\\n",
    "                -r $TAO_EXPERIMENT_DIR/resnet18 \\\n",
    "                -n resnet18 \\\n",
    "                -m $TAO_EXPERIMENT_DIR/pretrained_resnet18/pretrained_semantic_segmentation_vresnet18/resnet_18.hdf5 \\\n",
    "                -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad44218-d756-4e76-883f-0fcc76044254",
   "metadata": {},
   "source": [
    "**Note**: The training may take hours to complete. `unet` supports restarting from checkpoints in case the training job is killed prematurely. Training from the closest checkpoint may be resumed by simply re-running the **same** command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72ac48b2-d38e-482c-bb73-df6439dbc166",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for every epoch at checkpoint_interval mentioned in the spec file:\n",
      "---------------------\n",
      "\u001b[01;34m/dli/task/tao_project/unet/resnet18\u001b[00m\n",
      "└── output.log\n",
      "\n",
      "0 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "print('Model for every epoch at checkpoint_interval mentioned in the spec file:')\n",
    "print('---------------------')\n",
    "!tree -a $LOCAL_EXPERIMENT_DIR/resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc293375-1728-49ef-a331-a7d949ee6229",
   "metadata": {},
   "source": [
    "<a name='s2-2.7'></a>\n",
    "### Evaluating the Model ###\n",
    "The model should be evaluated for its performance at the end of training. The last step model saved in the `$USER_EXPERIMENT_DIR/resnet18/weights` directory is used for evaluation, inference, and export. The evaluation metrics include **precision**, **recall**, **f1-score**, and **IOU** for every class. The evaluation also creates `results_tlt.json` as a record. We can evaluate the model with the `evaluate` subtask. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "fefc75a1-ce72-4f4f-9f78-ef8035bbbe03",
   "metadata": {},
   "source": [
    "tao unet evaluate [-h] -e <EXPERIMENT_SPEC_FILE>\n",
    "                       -m <MODEL_PATH>\n",
    "                       -o <OUTPUT_DIR>\n",
    "                       -k <KEY>\n",
    "                       [--gpus GPUS]\n",
    "                       [--gpu_index GPU_INDEX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9d887-c398-4a6d-8c5a-e2e5629782a7",
   "metadata": {},
   "source": [
    "The `evaluate` subtask runs evaluation on the same validation set that was used during training. We can also run evaluation on an earlier model by editing the spec file to point to the intended model. When using the `evaluate` subtask, the `-e` argument indicates the path to the spec file, the `-m` argument indicates the path to the model, the `-o` argument indicates where the evaluation metrics outputs should be written, and the `-k` argument indicates the key to _load_ the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb363405-9ef2-4358-ac31-d80c692c2c8e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-24 16:48:11,763 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-09-24 16:48:11,922 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-tf:v3.21.11-tf1.15.5-py3\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/evaluate.py:43: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/evaluate.py:43: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "2023-09-24 16:48:16,596 [INFO] __main__: Loading experiment spec at /workspace/tao-experiments/spec_files/resnet18/combined_config.txt.\n",
      "2023-09-24 16:48:16,596 [INFO] iva.unet.spec_handler.spec_loader: Merging specification from /workspace/tao-experiments/spec_files/resnet18/combined_config.txt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1439, in ConsumeBool\n",
      "    result = ParseBool(self.token)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1756, in ParseBool\n",
      "    raise ValueError('Expected \"true\" or \"false\".')\n",
      "ValueError: Expected \"true\" or \"false\".\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/evaluate.py\", line 396, in <module>\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/evaluate.py\", line 392, in main\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/evaluate.py\", line 295, in run_experiment\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/spec_handler/spec_loader.py\", line 68, in load_experiment_spec\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/spec_handler/spec_loader.py\", line 48, in load_proto\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/spec_handler/spec_loader.py\", line 34, in _load_from_file\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 725, in Merge\n",
      "    allow_unknown_field=allow_unknown_field)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 793, in MergeLines\n",
      "    return parser.MergeLines(lines, message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 818, in MergeLines\n",
      "    self._ParseOrMerge(lines, message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 837, in _ParseOrMerge\n",
      "    self._MergeField(tokenizer, message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 967, in _MergeField\n",
      "    merger(tokenizer, message, field)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1042, in _MergeMessageField\n",
      "    self._MergeField(tokenizer, sub_message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 967, in _MergeField\n",
      "    merger(tokenizer, message, field)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1091, in _MergeScalarField\n",
      "    value = tokenizer.ConsumeBool()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1441, in ConsumeBool\n",
      "    raise self.ParseError(str(e))\n",
      "google.protobuf.text_format.ParseError: 3:12 : '  augment: <<<<FIXME>>>>': Expected \"true\" or \"false\".\n",
      "2023-09-24 16:48:17,390 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# evaluate the model using the same validation set as training\n",
    "!tao unet evaluate -e $TAO_SPECS_DIR/resnet18/combined_config.txt\\\n",
    "                   -m $TAO_EXPERIMENT_DIR/resnet18/weights/resnet18.tlt \\\n",
    "                   -o $TAO_EXPERIMENT_DIR/resnet18/ \\\n",
    "                   -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12815e-01d8-446d-bde2-28b82e3fc923",
   "metadata": {},
   "source": [
    "To understand how the TAO Toolkit measures accuracy of the segmentation model, we'll have to understand two measures: **recall** and **precision**. The first measure is focused on identifying positive cases and is called recall. We define recall as the ability of the model to identify all true positive samples of the dataset. In mathematical terms, recall is the ratio of true positives over true positives plus false negatives. By other means, recall tells us, among all the test samples belonging to the output class, how many of them are identified correctly by the model. The next measure is called precision and is defined as the ability of the model to identify the relevant samples only. It is the ratio of true positives over true positives plus false positives. A well-known measure that summarizes the balance between precision and recall is **f1-score**, which is their harmonic mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0319a-b49c-4dd8-94b2-795c30bed000",
   "metadata": {},
   "source": [
    "<a name='s2-2.8'></a>\n",
    "### Visualizing Model Inference ###\n",
    "The `inference` subtask for `unet` may be used to visualize segmentation and generate frame-by-frame PNG format labels on a directory of images. The labels will be generated in `mask_labels_tlt`. The tool also automatically generates segmentation overlayed images in `vis_overlay_tlt`. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d710380-3444-4947-ab09-e84711b94ed2",
   "metadata": {},
   "source": [
    "tao unet inference [-h] -e <EXPERIMENT_SPEC_FILE>\n",
    "                        -m <MODEL_PATH>\n",
    "                        -o <OUTPUT_DIR>\n",
    "                        -k <KEY>\n",
    "                        [--gpus GPUS]\n",
    "                        [--gpu_index GPU_INDEX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7201565-fe89-4335-ae26-2e8c3f765950",
   "metadata": {},
   "source": [
    "When using the `inference` subtask, the `-e` argument indicates the path to the inference spec file, the `-m` argument indicates the path to the model file, the `-o` argument indicates the path to the output images directory, and the `-k` argument indicates the key to _load_ the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c38936aa-7709-405f-b655-6059f90670a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# remove any previous inference\n",
    "!rm -rf $LOCAL_PROJECT_DIR/tao_infer_testing/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623818b-9146-482a-97eb-1b76458c2f38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-24 16:51:21,289 [INFO] root: Registry: ['nvcr.io']\n",
      "2023-09-24 16:51:21,428 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-tf:v3.21.11-tf1.15.5-py3\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/inference.py:45: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/inference.py:45: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "2023-09-24 16:51:26,086 [INFO] __main__: Loading experiment spec at /workspace/tao-experiments/spec_files/resnet18/combined_config.txt.\n",
      "2023-09-24 16:51:26,086 [INFO] iva.unet.spec_handler.spec_loader: Merging specification from /workspace/tao-experiments/spec_files/resnet18/combined_config.txt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1439, in ConsumeBool\n",
      "    result = ParseBool(self.token)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1756, in ParseBool\n",
      "    raise ValueError('Expected \"true\" or \"false\".')\n",
      "ValueError: Expected \"true\" or \"false\".\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/inference.py\", line 420, in <module>\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/inference.py\", line 416, in main\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/scripts/inference.py\", line 321, in run_experiment\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/spec_handler/spec_loader.py\", line 68, in load_experiment_spec\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/spec_handler/spec_loader.py\", line 48, in load_proto\n",
      "  File \"/root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/unet/spec_handler/spec_loader.py\", line 34, in _load_from_file\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 725, in Merge\n",
      "    allow_unknown_field=allow_unknown_field)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 793, in MergeLines\n",
      "    return parser.MergeLines(lines, message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 818, in MergeLines\n",
      "    self._ParseOrMerge(lines, message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 837, in _ParseOrMerge\n",
      "    self._MergeField(tokenizer, message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 967, in _MergeField\n",
      "    merger(tokenizer, message, field)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1042, in _MergeMessageField\n",
      "    self._MergeField(tokenizer, sub_message)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 967, in _MergeField\n",
      "    merger(tokenizer, message, field)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1091, in _MergeScalarField\n",
      "    value = tokenizer.ConsumeBool()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/google/protobuf/text_format.py\", line 1441, in ConsumeBool\n",
      "    raise self.ParseError(str(e))\n",
      "google.protobuf.text_format.ParseError: 3:12 : '  augment: <<<<FIXME>>>>': Expected \"true\" or \"false\".\n",
      "2023-09-24 16:51:26,877 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# perform inference on the validation set\n",
    "!tao unet inference -e $TAO_SPECS_DIR/resnet18/combined_config.txt \\\n",
    "                    -m $TAO_EXPERIMENT_DIR/resnet18/weights/resnet18.tlt \\\n",
    "                    -o $TAO_PROJECT_DIR/tao_infer_testing \\\n",
    "                    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b47fee3-fb55-4f1b-895d-9a2343061ffc",
   "metadata": {},
   "source": [
    "We can write a quick function that will help us sample random inferences. Execute the below cells to visualize the inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b44f4432-7f6b-4f08-be88-5c7eee82e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# define simple grid visualizer\n",
    "def visualize_images(num_images=10):\n",
    "    overlay_path=os.path.join(os.environ['LOCAL_PROJECT_DIR'], 'tao_infer_testing', 'vis_overlay_tlt')\n",
    "    inference_path=os.path.join(os.environ['LOCAL_PROJECT_DIR'], 'tao_infer_testing', 'mask_labels_tlt')\n",
    "    actual_path=os.path.join(os.environ['LOCAL_DATA_DIR'], 'masks', 'val')\n",
    "    inference_images_path=os.path.join(os.environ['LOCAL_DATA_DIR'], 'images', 'val')\n",
    "        \n",
    "    fig_dim=4\n",
    "    fig, ax_arr=plt.subplots(num_images, 4, figsize=[4*fig_dim, num_images*fig_dim], sharex=True, sharey=True)\n",
    "    ax_arr[0, 0].set_title('Overlay')\n",
    "    ax_arr[0, 1].set_title('Input')\n",
    "    ax_arr[0, 2].set_title('Inference')\n",
    "    ax_arr[0, 3].set_title('Actual')\n",
    "    ax_arr[0, 0].set_xticks([])\n",
    "    ax_arr[0, 0].set_yticks([])\n",
    "    \n",
    "    for idx, img_name in enumerate(random.sample(os.listdir(actual_path), num_images)):\n",
    "        ax_arr[idx, 0].imshow(plt.imread(overlay_path+'/'+img_name))\n",
    "        ax_arr[idx, 0].set_ylabel(img_name)\n",
    "        ax_arr[idx, 1].imshow(plt.imread(inference_images_path+'/'+img_name))\n",
    "        ax_arr[idx, 2].imshow(plt.imread(inference_path+'/'+img_name), cmap='gray')\n",
    "        ax_arr[idx, 3].imshow(plt.imread(actual_path+'/'+img_name), cmap='gray')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c340185d-c1c8-4051-8998-222f1b9b102e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/dli/task/tao_project/tao_infer_testing/vis_overlay_tlt/India_981708.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DO NOT CHANGE THIS CELL\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# visualizing random images\u001b[39;00m\n\u001b[1;32m      3\u001b[0m NUM_IMAGES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mvisualize_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNUM_IMAGES\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [21], line 23\u001b[0m, in \u001b[0;36mvisualize_images\u001b[0;34m(num_images)\u001b[0m\n\u001b[1;32m     20\u001b[0m ax_arr[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_yticks([])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, img_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(random\u001b[38;5;241m.\u001b[39msample(os\u001b[38;5;241m.\u001b[39mlistdir(actual_path), num_images)):\n\u001b[0;32m---> 23\u001b[0m     ax_arr[idx, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverlay_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mimg_name\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m     ax_arr[idx, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_ylabel(img_name)\n\u001b[1;32m     25\u001b[0m     ax_arr[idx, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(plt\u001b[38;5;241m.\u001b[39mimread(inference_images_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mimg_name))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py:2158\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimread)\n\u001b[1;32m   2157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(fname, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 2158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/image.py:1560\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1558\u001b[0m                 response \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(response\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m   1559\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m imread(response, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mext)\n\u001b[0;32m-> 1560\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimg_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[1;32m   1561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[1;32m   1562\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mPngImagePlugin\u001b[38;5;241m.\u001b[39mPngImageFile) \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1563\u001b[0m             pil_to_array(image))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/ImageFile.py:104\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodermaxblock \u001b[38;5;241m=\u001b[39m MAXBLOCK\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# filename\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/dli/task/tao_project/tao_infer_testing/vis_overlay_tlt/India_981708.png'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABPAAAAT/CAYAAACLlQzWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8CklEQVR4nO3de5BX9X3/8fcuMaCy4A0NyP4EUVskGqM2WKPBxKQk8RJvkTCKN+ItId6qaf9o8ZImNV7jTKIYY+ONFiiiJZrGSQQdY62X2KJSHS+FjLqOiqi7atSR/f7+cHYnG1DhWwyvLI/HjDPZ8z3nfD+bP95zeH7P92xLo9FoFAAAAAAQqXVdLwAAAAAAeG8CHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIe68Q111xTLS0ttXTp0nW9FAAAquq1116rr3/96/Wxj32sWlpa6rTTTlvXSwJgNbS0tNQ555yzrpfBh0zAWw8sXry4jjzyyNp6661r4MCBNWLEiDriiCNq8eLF63ppAFF6Plx44IEH1vVS6o033qhzzjmn7rjjjnW9FOBPyP9ljn3ve9+ra665pk4++eS6/vrra8qUKR/CCgHyXX755dXS0lLjx49v6viOjo4655xz6r//+7/X7sJYr31kXS+AD9e8efNq8uTJtdlmm9XUqVNr9OjRtXTp0rr66qtr7ty5NWvWrDr44IPX9TIB+ANvvPFGnXvuuVVVtc8++6zbxQDrhQULFtQee+xRZ5999rpeCsA6NXPmzBo1alTdd9999eSTT9Z22223Rsd3dHTUueeeW6NGjapddtnlw1kk6x134PVjTz31VE2ZMqW23Xbbeuihh+of/uEfaurUqfWd73ynHnroodp2221rypQp9b//+79/tDW9/vrrf7T3AgBg9b3wwgu1ySabrLXzdXd315tvvrnWzgfwx7BkyZL6j//4j7rkkktq2LBhNXPmzHW9JKgqAa9fu/DCC+uNN96oH//4xzVs2LA+r22xxRZ15ZVX1uuvv14XXHBBzZ07t1paWurOO+9c6TxXXnlltbS01COPPNK77bHHHqvDDjusNttssxo0aFDtvvvuNX/+/D7H9XyF484776xvfOMbteWWW9bIkSPfc73/9m//Vvvtt1+NGDGiBg4cWGPGjKnvfOc7tWLFit59zj777Npggw3qxRdfXOn4E044oTbZZBMXisBac8wxx9TgwYPr2WefrYMOOqgGDx5cw4YNqzPPPLPPbFq6dGm1tLTURRddVJdeemlts802teGGG9aECRP6zM6qd++mW9Uddcccc0yNGjWq93w9c/vcc8+tlpYWzzYBmrI6c+yOO+6olpaWWrJkSd166629M6fnWcVvvfVWnX322bXddtvVwIEDq729vb797W/XW2+91ee9Wlpaatq0aTVz5swaN25cDRw4sH7xi19UVdWzzz5bxx13XG211VY1cODAGjduXP3TP/1Tn+N71jFnzpz67ne/WyNHjqxBgwbVvvvuW08++eRKv9u9995bX/7yl2vTTTetjTfeuHbeeee67LLL+uyzOtesAL9v5syZtemmm9Z+++1Xhx122CoD3iuvvFKnn356jRo1qgYOHFgjR46so446qpYtW1Z33HFH/cVf/EVVVR177LG9M/Waa66pqqpRo0bVMcccs9I5//Aa8e23367p06fXbrvtVkOHDq2NN9649t5771q4cOGH8WvzJ8BXaPuxn/3sZzVq1Kjae++9V/n6Zz7zmRo1alTdeuutdemll9bgwYNrzpw5NWHChD77zZ49u8aNG1cf//jHq+rdZ+p9+tOfrq233rr+9m//tjbeeOOaM2dOHXTQQXXjjTeu9JXcb3zjGzVs2LCaPn36+96Bd80119TgwYPrjDPOqMGDB9eCBQtq+vTp1dnZWRdeeGFVVU2ZMqXOO++8mj17dk2bNq332Lfffrvmzp1bhx56aA0aNKip/78AVmXFihU1ceLEGj9+fF100UX1q1/9qi6++OIaM2ZMnXzyyX32ve6666qrq6u++c1v1ptvvlmXXXZZfe5zn6uHH364ttpqq9V+z2HDhtUVV1xRJ598ch188MF1yCGHVFXVzjvvvFZ/N2D98EFzbOzYsXX99dfX6aefXiNHjqy//uu/rqp3Z1F3d3cdeOCB9etf/7pOOOGEGjt2bD388MN16aWX1uOPP14333xzn/dasGBBzZkzp6ZNm1ZbbLFFjRo1qp5//vnaY489egPfsGHD6t///d9r6tSp1dnZudIfyzj//POrtbW1zjzzzHr11VfrggsuqCOOOKLuvffe3n1++ctf1v7771/Dhw+vU089tT72sY/Vo48+WrfcckudeuqpVbXm16wAVe8GvEMOOaQ++tGP1uTJk+uKK66o+++/vzfKvfbaa7X33nvXo48+Wscdd1ztuuuutWzZspo/f34988wzNXbs2DrvvPNq+vTpdcIJJ/T+e3zPPfdco3V0dnbWT37yk5o8eXIdf/zx1dXVVVdffXVNnDix7rvvPl/NXR816JdeeeWVRlU1vvKVr7zvfgceeGCjqhqdnZ2NyZMnN7bccsvGO++80/v6c88912htbW2cd955vdv23Xffxk477dR48803e7d1d3c39txzz8b222/fu+2nP/1po6oae+21V59z/v5rS5Ys6d32xhtvrLS+E088sbHRRhv1ea+//Mu/bIwfP77PfvPmzWtUVWPhwoXv+/sCvJ+e2XT//fc3Go1G4+ijj25UVZ8Z2Gg0Gp/85Ccbu+22W+/PS5YsaVRVY8MNN2w888wzvdvvvffeRlU1Tj/99N5tEyZMaEyYMGGl9z766KMb22yzTe/PL774YqOqGmefffba+eWA9UKzc6zRaDS22Wabxn777ddn2/XXX99obW1t3HXXXX22z5gxo1FVjbvvvrt3W1U1WltbG4sXL+6z79SpUxvDhw9vLFu2rM/2r33ta42hQ4f2XgMuXLiwUVWNsWPHNt56663e/S677LJGVTUefvjhRqPRaLzzzjuN0aNHN7bZZpvGyy+/3Oec3d3dvf97da9ZAXo88MADjapq/PKXv2w0Gu/OjJEjRzZOPfXU3n2mT5/eqKrGvHnzVjq+Zwbdf//9japq/PSnP11pn2222aZx9NFHr7T9D68R33nnnT6zsNFoNF5++eXGVltt1TjuuOP6bHfNuH7wFdp+qqurq6qq2tra3ne/ntc7Oztr0qRJ9cILL/T5i4dz586t7u7umjRpUlVVLV++vBYsWFCHH354dXV11bJly2rZsmX10ksv1cSJE+uJJ56oZ599ts97HH/88TVgwIAPXPOGG27YZ/3Lli2rvffeu95444167LHHel876qij6t57762nnnqqd9vMmTOrvb19pbsHAdaGk046qc/Pe++99yqfH3rQQQfV1ltv3fvzpz71qRo/fnz9/Oc//9DXCPB+VneO/aF//dd/rbFjx9af//mf9173LVu2rD73uc9VVa30Va4JEybUjjvu2Ptzo9GoG2+8sQ444IBqNBp9zjFx4sR69dVX68EHH+xzjmOPPbY++tGP9llrVfWu97/+679qyZIlddppp630zL6Wlpaqau6aFWDmzJm11VZb1Wc/+9mqenemTJo0qWbNmtX72IEbb7yxPvGJT6zyLt6eGbQ2DBgwoHcWdnd31/Lly+udd96p3XfffaW5yfpBwOunesJcT8h7L78f+r74xS/W0KFDa/bs2b2vz549u3bZZZfaYYcdqqrqySefrEajUX//939fw4YN6/Nfz18se+GFF/q8x+jRo1drzYsXL66DDz64hg4dWkOGDKlhw4bVkUceWVVVr776au9+kyZNqoEDB/Y+i+DVV1+tW265pY444oi1OjABqqoGDRq00nNEN91003r55ZdX2nf77bdfadsOO+zQ+xwpgHVhTebYH3riiSdq8eLFK1339VwbftB134svvlivvPJK7zOZf/+/Y489dpXn+H//7/+ttNaq6l1vz4e4PY93WZVmrlmB9duKFStq1qxZ9dnPfraWLFlSTz75ZD355JM1fvz4ev755+v222+vqndn0PvNn7Xp2muvrZ133rkGDRpUm2++eQ0bNqxuvfXWPv8+Zv3hGXj91NChQ2v48OH10EMPve9+Dz30UG299dY1ZMiQqnr37pGbbrqpLr/88nr++efr7rvvru9973u9+3d3d1dV1ZlnnlkTJ05c5Tn/8E9s//6dde/llVdeqQkTJtSQIUPqvPPOqzFjxtSgQYPqwQcfrL/5m7/pfd+qdy/i9t9//5o5c2ZNnz695s6dW2+99VZv7ANYm1bnDuI10dLSUo1GY6Xtv/9HMQDWpv/LHOvu7q6ddtqpLrnkklW+3t7e3ufnP7zu67mGO/LII+voo49e5Tn+8Pme77XeVc3O99LMNSuwfluwYEE999xzNWvWrJo1a9ZKr8+cObP+6q/+6v/8Pu9108mKFSv6zL8bbrihjjnmmDrooIPqrLPOqi233LIGDBhQ//iP/9jn22isPwS8fmz//fevq666qn7961/XXnvttdLrd911Vy1durROPPHE3m2TJk2qa6+9tm6//fZ69NFHq9Fo9H59tqpq2223raqqDTbYoD7/+c+vtbXecccd9dJLL9W8efPqM5/5TO/2JUuWrHL/o446qr7yla/U/fffXzNnzqxPfvKTNW7cuLW2HoBmPPHEEytte/zxx3v/umzVux9CrOpra7/97W/7/OyOYiDBmDFjatGiRbXvvvs2NZeGDRtWbW1ttWLFirV27ThmzJiqqnrkkUfe85wf1jUr0H/NnDmzttxyy/rRj3600mvz5s2rm266qWbMmFFjxoypRx555H3P9X7zctNNN61XXnllpe2//e1ve2dX1buPs9p2221r3rx5fc7Xcxcx6x9foe3HzjrrrNpwww3rxBNPrJdeeqnPa8uXL6+TTjqpNtpoozrrrLN6t3/+85+vzTbbrGbPnl2zZ8+uT33qU32+CrHlllvWPvvsU1deeWU999xzK73niy++2NRaez5p+P1PVt9+++26/PLLV7n/l770pdpiiy3q+9//ft15553uvgMi3HzzzX2eqXTffffVvffeW1/60pd6t40ZM6Yee+yxPvNy0aJFdffdd/c510YbbVRVtcoLPIA/lsMPP7yeffbZuuqqq1Z67Xe/+129/vrr73v8gAED6tBDD60bb7xxlf/gbebacdddd63Ro0fXD37wg5VmZM+15Id1zQr0T7/73e9q3rx5tf/++9dhhx220n/Tpk2rrq6umj9/fh166KG1aNGiuummm1Y6T88M2njjjatq1ddxY8aMqf/8z/+st99+u3fbLbfcUk8//XSf/Vb1b+R777237rnnnv/z78ufJnfg9WPbb799XXvttXXEEUfUTjvtVFOnTq3Ro0fX0qVL6+qrr65ly5bVv/zLv/R+iln17qeUhxxySM2aNatef/31uuiii1Y6749+9KPaa6+9aqeddqrjjz++tt1223r++efrnnvuqWeeeaYWLVq0xmvdc889a9NNN62jjz66TjnllGppaanrr7/+Pb8qscEGG9TXvva1+uEPf1gDBgyoyZMnr/F7Aqxt2223Xe2111518skn11tvvVU/+MEPavPNN69vf/vbvfscd9xxdckll9TEiRNr6tSp9cILL9SMGTNq3Lhx1dnZ2bvfhhtuWDvuuGPNnj27dthhh9pss83q4x//+B/tmSsAVVVTpkypOXPm1EknnVQLFy6sT3/607VixYp67LHHas6cOXXbbbfV7rvv/r7nOP/882vhwoU1fvz4Ov7442vHHXes5cuX14MPPli/+tWvavny5Wu0ptbW1rriiivqgAMOqF122aWOPfbYGj58eD322GO1ePHiuu2226rqw7lmBfqn+fPnV1dXVx144IGrfH2PPfaoYcOG1cyZM+uf//mfa+7cufXVr361jjvuuNptt91q+fLlNX/+/JoxY0Z94hOfqDFjxtQmm2xSM2bMqLa2ttp4441r/PjxNXr06Pr6179ec+fOrS9+8Yt1+OGH11NPPVU33HBDn3+XV737jbp58+bVwQcfXPvtt18tWbKkZsyYUTvuuGO99tprf4z/WwjjDrx+7qtf/Wr95je/qX322aeuvvrqOumkk+qqq66qCRMm1G9+85s65JBDVjpm0qRJvQPh8MMPX+n1HXfcsR544IHab7/96pprrqlvfvObNWPGjGptba3p06c3tc7NN9+8brnllho+fHj93d/9XV100UX1hS98oS644IL3POaoo46qqqp99923hg8f3tT7AqxNRx11VH3rW9+qH/7wh/Xd7363xo0bVwsWLOgzo8aOHVvXXXddvfrqq3XGGWfU/Pnz6/rrr69dd911pfP95Cc/qa233rpOP/30mjx5cs2dO/eP+esAVGtra9188811/vnn18MPP1xnnnlmnXvuuXX//ffXqaee2vvHLN7PVlttVffdd18de+yxNW/evJo2bVpddtlltXz58vr+97/f1LomTpxYCxcurB122KEuvvjiOuOMM+r222+vAw44oHefD+OaFeifZs6cWYMGDaovfOELq3y9tbW19ttvv/rFL35Rb731Vt1111118skn189//vM65ZRT6vLLL68/+7M/q5EjR1bVuzecXHvttTVgwIA66aSTavLkyXXnnXdW1bvz6+KLL67HH3+8TjvttLrnnnvqlltu6T22xzHHHFPf+973atGiRXXKKafUbbfdVjfccMMHfmhC/9XSWJOnwUKQRYsW1S677FLXXXddTZkyZV0vB1iPLV26tEaPHl0XXnhhnXnmmet6OQAAQD/jDjz+ZF111VU1ePDgVd5FCAAAANBfeAYef3J+9rOf1f/8z//Uj3/845o2bVrvA0IBAAAA+iMBjz853/rWt+r555+vL3/5y3Xuueeu6+UAAAAAfKg8Aw8AAAAAgnkGHgAAAAAEE/AAAAAAIFjTz8Dr7u6ujo6Oamtrq5aWlrW5JqAfajQa1dXVVSNGjKjW1v7z2YFZCKwp8xDALATosbrzsOmA19HRUe3t7c0eDqynnn766Ro5cuS6XsZaYxYCzTIPAcxCgB4fNA+bDnhtbW29bzBkyJBmTwOsJzo7O6u9vb13dvQXZiGwpsxDALMQoMfqzsOmA17P7cBDhgwxmIDV1t++SmAWAs0yDwHMQoAeHzQP+8/DBgAAAACgHxLwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgH2n2wEajUVVVnZ2da20xQP/VMyt6Zkd/YRYCa8o8BDALAXqs7jxsOuB1dXVVVVV7e3uzpwDWQ11dXTV06NB1vYy1xiwEmmUeApiFAD0+aB62NJr8yKO7u7s6Ojqqra2tWlpaml4gsH5oNBrV1dVVI0aMqNbW/vPtfbMQWFPmIYBZCNBjdedh0wEPAAAAAPjw9Z+POgAAAACgHxLwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACDYR5o9sLu7uzo6Oqqtra1aWlrW5pqAfqjRaFRXV1eNGDGiWlv7z2cHZiGwpsxDALMQoMfqzsOmA15HR0e1t7c3eziwnnr66adr5MiR63oZa41ZCDTLPAQwCwF6fNA8bDrgtbW19b7BkCFDmj0NsJ7o7Oys9vb23tnRX5iFwJoyDwHMQoAeqzsPmw54PbcDDxkyxGACVlt/+yqBWQg0yzwEMAsBenzQPOw/DxsAAAAAgH5IwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAECwjzR7YKPRqKqqzs7OtbYYoP/qmRU9s6O/MAuBNWUeApiFAD1Wdx42HfC6urqqqqq9vb3ZUwDroa6urho6dOi6XsZaYxYCzTIPAcxCgB4fNA9bGk1+5NHd3V0dHR3V1tZWLS0tTS8QWD80Go3q6uqqESNGVGtr//n2vlkIrCnzEMAsBOixuvOw6YAHAAAAAHz4+s9HHQAAAADQDwl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABDsI80e2N3dXR0dHdXW1lYtLS1rc01AP9RoNKqrq6tGjBhRra3957MDsxBYU+YhgFkI0GN152HTAa+jo6Pa29ubPRxYTz399NM1cuTIdb2MtcYsBJplHgKYhQA9PmgeNh3w2traet9gyJAhzZ4GWE90dnZWe3t77+zoL8xCYE2ZhwBmIUCP1Z2HTQe8ntuBhwwZYjABq62/fZXALASaZR4CmIUAPT5oHvafhw0AAAAAQD8k4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwT7S7IGNRqOqqjo7O9faYoD+q2dW9MyO/sIsBNaUeQhgFgL0WN152HTA6+rqqqqq9vb2Zk8BrIe6urpq6NCh63oZa41ZCDTLPAQwCwF6fNA8bGk0+ZFHd3d3dXR0VFtbW7W0tDS9QGD90Gg0qqurq0aMGFGtrf3n2/tmIbCmzEMAsxCgx+rOw6YDHgAAAADw4es/H3UAAAAAQD8k4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAj2kWYP7O7uro6Ojmpra6uWlpa1uSagH2o0GtXV1VUjRoyo1tb+89mBWQisKfMQwCwE6LG687DpgNfR0VHt7e3NHg6sp55++ukaOXLkul7GWmMWAs0yDwHMQoAeHzQPmw54bW1tvW8wZMiQZk8DrCc6Ozurvb29d3b0F2YhsKbMQwCzEKDH6s7DpgNez+3AQ4YMMZiA1dbfvkpgFgLNMg8BzEKAHh80D/vPwwYAAAAAoB8S8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYAIeAAAAAAQT8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAgm4AEAAABAMAEPAAAAAIIJeAAAAAAQTMADAAAAgGACHgAAAAAEE/AAAAAAIJiABwAAAADBBDwAAAAACCbgAQAAAEAwAQ8AAAAAggl4AAAAABBMwAMAAACAYB9p9sBGo1FVVZ2dnWttMUD/1TMremZHf2EWAmvKPAQwCwF6rO48bDrgdXV1VVVVe3t7s6cA1kNdXV01dOjQdb2MtcYsBJplHgKYhQA9PmgetjSa/Miju7u7Ojo6qq2trVpaWppeILB+aDQa1dXVVSNGjKjW1v7z7X2zEFhT5iGAWQjQY3XnYdMBDwAAAAD48PWfjzoAAAAAoB8S8AAAAAAgmIAHAAAAAMEEPAAAAAAIJuABAAAAQDABDwAAAACCCXgAAAAAEEzAAwAAAIBgAh4AAAAABBPwAAAAACCYgAcAAAAAwQQ8AAAAAAj2/wF3kGv3EJtFIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1600x1600 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# visualizing random images\n",
    "NUM_IMAGES = 4\n",
    "\n",
    "visualize_images(NUM_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fdbad0-41a0-4fd7-b09f-6d8dabf5c9fc",
   "metadata": {},
   "source": [
    "<a name='s2-3'></a>\n",
    "## Model Export ##\n",
    "Once we are satisfied with our model, we can move to deployment. `unet` includes an `export` subtask to export and prepare a trained U-Net model for deployment. Exporting the model decouples the training process from deployment and allows conversion to TensorRT engines outside the TAO environment. TensorRT is a highly optimized package that takes trained models and optimizes them for inference. TensorRT engines are specific to each hardware configuration and should be generated for each unique inference environment. This may be interchangeably referred to as the `.trt` or `.engine` file. The same exported TAO model may be used universally across training and deployment hardware. This is referred to as the `.etlt` file, or encrypted TAO file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8639a37-72d6-4e0f-95f4-d3de568f03a0",
   "metadata": {},
   "source": [
    "<a name='s2-3.1'></a>\n",
    "### TensorRT - Programmable Inference Accelerator\n",
    "\n",
    "NVIDIA [TensorRT](https://developer.nvidia.com/tensorrt) is a platform for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning inference applications. TensorRT-based applications perform up to 40x faster than CPU-only platforms during inference. \n",
    "\n",
    "With TensorRT, we can optimize neural network models trained in all major frameworks, calibrate for lower precision with high accuracy, and finally deploy to hyperscale data centers, embedded, or automotive product platforms.\n",
    "\n",
    "How TensorRT enables optimizations on the layer graph: \n",
    "* Elimination of layers whose outputs are not used\n",
    "* Fusion of convolution, bias, and ReLU operations\n",
    "* Aggregation of operations with sufficiently similar parameters and the same source tensor (for example, the 1x1 convolutions in GoogleNet’ s inception module)\n",
    "* Merging of concatenation layers by directing layer outputs to the correct eventual destination\n",
    "\n",
    "Here are some great resources to learn more about TensorRT:\n",
    " \n",
    "* Main Page: https://developer.nvidia.com/tensorrt\n",
    "* Blogs: https://devblogs.nvidia.com/speed-up-inference-tensorrt/\n",
    "* Download: https://developer.nvidia.com/nvidia-tensorrt-download\n",
    "* Documentation: https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html\n",
    "* Sample Code: https://docs.nvidia.com/deeplearning/sdk/tensorrt-sample-support-guide/index.html\n",
    "* GitHub: https://github.com/NVIDIA/TensorRT\n",
    "* NGC Container: https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1cd365-e7fd-4c8c-b03e-9d8c310b0e08",
   "metadata": {},
   "source": [
    "<a name='s2-3.2'></a>\n",
    "### Export the Trained Model ###\n",
    "When using the `export` subtask, the `-m` argument indicates the path to the `.tlt` model file to be exported, the `-e` argument indicates the path to the spec file, and `-k` argument indicates the key to _load_ the model. There are two optional arguments, `--gen_ds_config` and `--engine_file` that are useful for us. The `--gen_ds_config` argument indicates whether to generate a template inference configuration file as well as a label file. The `--engine_file` indicates the path to the serialized TensorRT engine file. \n",
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "Note that the TensorRT file is hardware specific and cannot be generalized across GPUs. Since a TensorRT engine file is hardware specific, you cannot use an engine file for deployment unless the deployment GPU is identical to the training GPU. This is true in our case since the Triton Inference Server will run on the same hardware. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "afb18b3b-86a5-4c4f-9296-a310516a8c8f",
   "metadata": {},
   "source": [
    "tao unet export [-h] -e <EXPERIMENT_SPEC>\n",
    "                     -m <MODEL>\n",
    "                     -k <KEY>\n",
    "                     [--engine_file ENGINE_FILE]\n",
    "                     [--gen_ds_config]\n",
    "                     [--gpus GPUS]\n",
    "                     [--gpu_index GPU_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcae38a-6f81-4818-b428-bd653e208680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# remove any previous exports if exists\n",
    "!rm $LOCAL_EXPERIMENT_DIR/resnet18/weights/resnet18.etlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7c99c-2576-499e-8b37-347a1c20d4a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# export model and TensorRT engine\n",
    "!tao unet export -m $TAO_EXPERIMENT_DIR/resnet18/weights/resnet18.tlt \\\n",
    "                 -e $TAO_SPECS_DIR/resnet18/combined_config.txt \\\n",
    "                 -k $KEY \\\n",
    "                 --engine_file $TAO_EXPERIMENT_DIR/export/resnet18.engine \\\n",
    "                 --gen_ds_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01280ca6-e986-4fe6-9ec5-366e8cf6df8a",
   "metadata": {},
   "source": [
    "**Well Done!** Let's move to the [next notebook](./03_model_deployment_for_inference.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e715988-e892-4565-bcb9-85c289eefd2d",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/combined_logo.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
